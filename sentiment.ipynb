{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification using naive bayes for sentiment analysis\n",
    "\n",
    "Please note that this is just done to understand what libraries to use and to\n",
    "get an understanding of the processes involved in text classification. We will \n",
    "have to adapt the model to classify sentences of reviews into various topics. \n",
    "The steps we take for our analysis are basically\n",
    "\n",
    "1. Classify sentences into topics e.g. service, fees, registration\n",
    "2. Classify sentences within each topic as positive or negative\n",
    "3. Count positive/negative within each topic\n",
    "4. Perform linear/multi-linear regression between pos/neg in each topic, and\n",
    "the rating associated the text that sentence belongs to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tutorial](https://www.youtube.com/watch?v=oq68P8Kv7nE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk #! Must download nltk punkt\n",
    "from nltk import word_tokenize # ALternatively use nlp() parser from spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fed official says weak data caused by weather,...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fed's Charles Plosser sees high bar for change...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US open: Stocks fall after Fed official hints ...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fed risks falling 'behind the curve', Charles ...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fed's Plosser: Nasty Weather Has Curbed Job Gr...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TITLE CATEGORY\n",
       "0  Fed official says weak data caused by weather,...        b\n",
       "1  Fed's Charles Plosser sees high bar for change...        b\n",
       "2  US open: Stocks fall after Fed official hints ...        b\n",
       "3  Fed risks falling 'behind the curve', Charles ...        b\n",
       "4  Fed's Plosser: Nasty Weather Has Curbed Job Gr...        b"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b - business, t - tech, e - entertainment.\n",
    "df = pd.read_csv(\"sentiment.csv\").loc[:, [\"TITLE\", \"CATEGORY\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "e    32\n",
       "t    22\n",
       "b    11\n",
       "Name: CATEGORY, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"CATEGORY\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pifort', 'Technologies', 'is', 'a', 'Software', 'Development', 'company', '.', 'Piford', 'also', 'provides', 'a', 'training', 'program', '.']\n"
     ]
    }
   ],
   "source": [
    "# Alt: Use Spacy nlp() to create tokens automatically (with lemmatization)\n",
    "data = \"Pifort Technologies is a Software Development company. Piford also provides a training program.\"\n",
    "print(word_tokenize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P', 'i', 'f', 'o', 'r', 't', ' ', 'T', 'e', 'c', 'h', 'n', 'o', 'l', 'o', 'g', 'i', 'e', 's', ' ', 'i', 's', ' ', 'a', ' ', 'S', 'o', 'f', 't', 'w', 'a', 'r', 'e', ' ', 'D', 'e', 'v', 'e', 'l', 'o', 'p', 'm', 'e', 'n', 't', ' ', 'c', 'o', 'm', 'p', 'a', 'n', 'y', ' ', 'P', 'i', 'f', 'o', 'r', 'd', ' ', 'a', 'l', 's', 'o', ' ', 'p', 'r', 'o', 'v', 'i', 'd', 'e', 's', ' ', 'a', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'p', 'r', 'o', 'g', 'r', 'a', 'm']\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation from token list () alternatively use token.lemma_ in nlp()\n",
    "import string\n",
    "data_1 = [char for char in data if char not in string.punctuation]\n",
    "print(data_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pifort Technologies is a Software Development company Piford also provides a training program\n"
     ]
    }
   ],
   "source": [
    "data_1 = \"\".join(data_1)\n",
    "print(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pifort', 'Technologies', 'is', 'a', 'Software', 'Development', 'company', 'Piford', 'also', 'provides', 'a', 'training', 'program']\n"
     ]
    }
   ],
   "source": [
    "data_1 = data_1.split()\n",
    "print(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pifort', 'Technologies', 'Software', 'Development', 'company', 'Piford', 'also', 'provides', 'training', 'program']\n"
     ]
    }
   ],
   "source": [
    "# delete all stop words from our data\n",
    "from nltk.corpus import stopwords\n",
    "data_1 = [word for word in data_1 if word not in stopwords.words(\"english\")];\n",
    "print(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pifort': 4, 'technologies': 8, 'software': 7, 'development': 2, 'company': 1, 'piford': 3, 'also': 0, 'provides': 6, 'training': 9, 'program': 5}\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction Convert words into feature vectors (bag of words variation\n",
    "# called the count vectoriser)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectoriser = CountVectorizer()\n",
    "vectoriser.fit(data_1)\n",
    "print(vectoriser.vocabulary_) # Shows the numeric value/index assigned to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 9)\t1\n"
     ]
    }
   ],
   "source": [
    "# Get bag of words count (index in document list, index index of word, frequency of word)\n",
    "data_1 = [\" \".join(data_1)]\n",
    "vector = vectoriser.transform(data_1)\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing for real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function we want ot apply to each document\n",
    "def text_cleaning(a):\n",
    "    remove_punctuation = [char for char in a if char not in string.punctuation]\n",
    "    remove_punctuation = \"\".join(remove_punctuation)\n",
    "    return [word for word in remove_punctuation.split() if word.lower() not in stopwords.words(\"english\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [Fed, official, says, weak, data, caused, weat...\n",
      "1     [Feds, Charles, Plosser, sees, high, bar, chan...\n",
      "2     [US, open, Stocks, fall, Fed, official, hints,...\n",
      "3     [Fed, risks, falling, behind, curve, Charles, ...\n",
      "4     [Feds, Plosser, Nasty, Weather, Curbed, Job, G...\n",
      "                            ...                        \n",
      "60    [GM, recalls, another, 24M, vehicles, belts, b...\n",
      "61    [Business, update, Parade, GM, recalls, rolls,...\n",
      "62                     [GM, keeps, recalling, vehicles]\n",
      "63                                        [GM, recalls]\n",
      "64                     [10, largest, GM, recalls, year]\n",
      "Name: TITLE, Length: 65, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Apply text clearning function to every element in series/column\n",
    "print(df.loc[:, \"TITLE\"].apply(text_cleaning)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fed': 61,\n",
       " 'official': 202,\n",
       " 'says': 221,\n",
       " 'weak': 243,\n",
       " 'data': 171,\n",
       " 'caused': 167,\n",
       " 'weather': 244,\n",
       " 'slow': 224,\n",
       " 'taper': 229,\n",
       " 'Feds': 62,\n",
       " 'Charles': 38,\n",
       " 'Plosser': 109,\n",
       " 'sees': 222,\n",
       " 'high': 183,\n",
       " 'bar': 160,\n",
       " 'change': 168,\n",
       " 'pace': 205,\n",
       " 'tapering': 230,\n",
       " 'US': 141,\n",
       " 'open': 203,\n",
       " 'Stocks': 131,\n",
       " 'fall': 176,\n",
       " 'hints': 184,\n",
       " 'accelerated': 155,\n",
       " 'risks': 219,\n",
       " 'falling': 177,\n",
       " 'behind': 162,\n",
       " 'curve': 170,\n",
       " 'Nasty': 96,\n",
       " 'Weather': 147,\n",
       " 'Curbed': 45,\n",
       " 'Job': 81,\n",
       " 'Growth': 73,\n",
       " 'May': 86,\n",
       " 'Accelerate': 18,\n",
       " 'Tapering': 136,\n",
       " 'Pace': 104,\n",
       " 'Taper': 135,\n",
       " 'may': 195,\n",
       " 'expects': 175,\n",
       " 'unemployment': 236,\n",
       " '62': 14,\n",
       " 'end': 174,\n",
       " '2014': 4,\n",
       " 'jobs': 190,\n",
       " 'growth': 182,\n",
       " 'last': 193,\n",
       " 'month': 197,\n",
       " 'hit': 185,\n",
       " 'weatherFed': 245,\n",
       " 'President': 113,\n",
       " 'ECB': 54,\n",
       " 'unlikely': 237,\n",
       " 'sterilisation': 225,\n",
       " 'SMP': 121,\n",
       " 'purchases': 207,\n",
       " 'traders': 235,\n",
       " 'sterilization': 226,\n",
       " 'Box': 32,\n",
       " 'Office': 102,\n",
       " 'XMen': 153,\n",
       " 'Days': 50,\n",
       " 'Future': 68,\n",
       " 'Past': 106,\n",
       " 'Nabs': 95,\n",
       " '261M': 11,\n",
       " 'Worldwide': 150,\n",
       " 'Debut': 51,\n",
       " 'Mania': 85,\n",
       " 'Report': 119,\n",
       " 'FUTURE': 59,\n",
       " 'RULES': 115,\n",
       " 'Believe': 24,\n",
       " 'Christian': 41,\n",
       " 'theme': 231,\n",
       " 'Review': 120,\n",
       " '–': 251,\n",
       " 'Sequel': 124,\n",
       " 'Prequel': 112,\n",
       " 'Reboot': 116,\n",
       " 'Numbers': 99,\n",
       " 'Slay': 127,\n",
       " 'Godzilla': 72,\n",
       " '7': 15,\n",
       " 'hours': 187,\n",
       " 'ago': 157,\n",
       " 'dominates': 173,\n",
       " 'holiday': 186,\n",
       " 'box': 166,\n",
       " 'office': 201,\n",
       " '91M': 17,\n",
       " 'Memorial': 87,\n",
       " 'Day': 49,\n",
       " 'weekend': 246,\n",
       " 'movie': 198,\n",
       " '“Days': 252,\n",
       " 'XMen”': 154,\n",
       " 'beat': 161,\n",
       " 'record': 214,\n",
       " '1': 1,\n",
       " 'million': 196,\n",
       " 'views': 242,\n",
       " 'global': 181,\n",
       " 'rakes': 208,\n",
       " 'Film': 63,\n",
       " 'reviews': 217,\n",
       " 'Bryan': 35,\n",
       " 'Singers': 126,\n",
       " '“XMen': 254,\n",
       " 'Past”': 107,\n",
       " 'returns': 216,\n",
       " 'familiar': 178,\n",
       " 'franchise': 179,\n",
       " 'themes': 232,\n",
       " 'rise': 218,\n",
       " 'reaching': 209,\n",
       " 'top': 233,\n",
       " 'Weekend': 148,\n",
       " 'Wrap': 151,\n",
       " '25th': 9,\n",
       " 'KWWL': 82,\n",
       " 'Eastern': 55,\n",
       " 'Iowa': 80,\n",
       " 'Breaking': 34,\n",
       " 'movies': 199,\n",
       " 'takes': 228,\n",
       " 'records': 215,\n",
       " 'Sets': 125,\n",
       " 'Franchise': 65,\n",
       " 'Best': 25,\n",
       " '262': 12,\n",
       " 'Mil': 88,\n",
       " 'PreOrder': 111,\n",
       " 'XMEN': 152,\n",
       " 'DAYS': 46,\n",
       " 'PAST': 103,\n",
       " 'Bluray': 28,\n",
       " 'Score': 122,\n",
       " '91': 16,\n",
       " 'debut': 172,\n",
       " 'Blended': 27,\n",
       " 'Moves': 93,\n",
       " 'Thats': 138,\n",
       " 'Boy': 33,\n",
       " 'Wins': 149,\n",
       " 'Foxs': 64,\n",
       " 'Topples': 139,\n",
       " 'VIDEO': 145,\n",
       " 'Gang': 70,\n",
       " 'Towers': 140,\n",
       " 'N': 94,\n",
       " 'America': 21,\n",
       " 'Holiday': 77,\n",
       " 'Colossal': 43,\n",
       " 'opening': 204,\n",
       " 'Extinction': 58,\n",
       " 'TV': 134,\n",
       " 'Spot': 130,\n",
       " 'Released': 118,\n",
       " 'INTL': 79,\n",
       " 'BOX': 23,\n",
       " 'OFFICE': 100,\n",
       " 'FanTastic': 60,\n",
       " 'Bows': 31,\n",
       " '…': 255,\n",
       " 'Dominates': 53,\n",
       " 'Million': 89,\n",
       " '3': 13,\n",
       " 'reasons': 210,\n",
       " 'best': 164,\n",
       " 'yet': 250,\n",
       " 'gang': 180,\n",
       " 'towers': 234,\n",
       " 'North': 98,\n",
       " 'Slays': 128,\n",
       " 'Bombs': 30,\n",
       " 'DOFP': 47,\n",
       " 'Unnuked': 142,\n",
       " 'Fridge': 66,\n",
       " '“Unnuked': 253,\n",
       " 'Fridge”': 67,\n",
       " 'IBM': 78,\n",
       " 'make': 194,\n",
       " 'big': 165,\n",
       " 'accessible': 156,\n",
       " 'users': 239,\n",
       " 'via': 241,\n",
       " 'cloud': 169,\n",
       " 'Bolsters': 29,\n",
       " 'Storage': 132,\n",
       " 'Play': 108,\n",
       " 'Ups': 144,\n",
       " 'x86': 248,\n",
       " 'Hardware': 75,\n",
       " 'Portfolio': 110,\n",
       " 'shell': 223,\n",
       " 'infrastructure': 188,\n",
       " 'study': 227,\n",
       " 'Cheap': 40,\n",
       " 'Chart': 39,\n",
       " 'Cloud': 42,\n",
       " 'Exposure': 57,\n",
       " 'Concerning': 44,\n",
       " 'Launches': 84,\n",
       " 'New': 97,\n",
       " 'Systems': 133,\n",
       " 'Offerings': 101,\n",
       " 'Big': 26,\n",
       " 'Data': 48,\n",
       " 'Unveils': 143,\n",
       " 'Software': 129,\n",
       " 'Defined': 52,\n",
       " 'Era': 56,\n",
       " 'Technology': 137,\n",
       " 'GM': 69,\n",
       " 'recalls': 213,\n",
       " 'another': 158,\n",
       " '24': 5,\n",
       " 'belts': 163,\n",
       " 'bags': 159,\n",
       " 'Parade': 105,\n",
       " 'rolls': 220,\n",
       " 'vehicles': 240,\n",
       " '0': 0,\n",
       " 'Minutes': 90,\n",
       " 'Ago': 20,\n",
       " '24M': 7,\n",
       " 'Recalls': 117,\n",
       " 'Another': 22,\n",
       " '26': 10,\n",
       " 'Vehicles': 146,\n",
       " 'issues': 189,\n",
       " 'new': 200,\n",
       " 'recall': 211,\n",
       " '242': 6,\n",
       " 'Mln': 91,\n",
       " 'Sees': 123,\n",
       " 'Addl': 19,\n",
       " 'Q2': 114,\n",
       " 'Charge': 37,\n",
       " '200': 3,\n",
       " 'Hit': 76,\n",
       " 'Lansing': 83,\n",
       " 'Hard': 74,\n",
       " 'General': 71,\n",
       " 'Motors': 92,\n",
       " 'parade': 206,\n",
       " '259': 8,\n",
       " 'worldwide': 247,\n",
       " 'Business': 36,\n",
       " 'update': 238,\n",
       " 'keeps': 191,\n",
       " 'recalling': 212,\n",
       " '10': 2,\n",
       " 'largest': 192,\n",
       " 'year': 249}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the vectoriser to learn the dictionary from the title series\n",
    "#NB: analyzer=text_clearning basically applies the function to each row in series\n",
    "bow_transformer = CountVectorizer(analyzer=text_cleaning).fit(df[\"TITLE\"])\n",
    "bow_transformer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 61)\t1\n",
      "  (0, 167)\t1\n",
      "  (0, 171)\t1\n",
      "  (0, 202)\t1\n",
      "  (0, 221)\t1\n",
      "  (0, 224)\t1\n",
      "  (0, 229)\t1\n",
      "  (0, 243)\t1\n",
      "  (0, 244)\t1\n",
      "  (1, 38)\t1\n",
      "  (1, 62)\t1\n",
      "  (1, 109)\t1\n",
      "  (1, 160)\t1\n",
      "  (1, 168)\t1\n",
      "  (1, 183)\t1\n",
      "  (1, 205)\t1\n",
      "  (1, 222)\t1\n",
      "  (1, 230)\t1\n",
      "  (2, 61)\t1\n",
      "  (2, 131)\t1\n",
      "  (2, 141)\t1\n",
      "  (2, 155)\t1\n",
      "  (2, 176)\t1\n",
      "  (2, 184)\t1\n",
      "  (2, 202)\t1\n",
      "  :\t:\n",
      "  (60, 69)\t1\n",
      "  (60, 158)\t1\n",
      "  (60, 159)\t1\n",
      "  (60, 163)\t1\n",
      "  (60, 213)\t1\n",
      "  (60, 240)\t1\n",
      "  (61, 7)\t1\n",
      "  (61, 36)\t1\n",
      "  (61, 69)\t1\n",
      "  (61, 105)\t1\n",
      "  (61, 213)\t1\n",
      "  (61, 220)\t1\n",
      "  (61, 238)\t1\n",
      "  (61, 240)\t1\n",
      "  (62, 69)\t1\n",
      "  (62, 191)\t1\n",
      "  (62, 212)\t1\n",
      "  (62, 240)\t1\n",
      "  (63, 69)\t1\n",
      "  (63, 213)\t1\n",
      "  (64, 2)\t1\n",
      "  (64, 69)\t1\n",
      "  (64, 192)\t1\n",
      "  (64, 213)\t1\n",
      "  (64, 249)\t1\n"
     ]
    }
   ],
   "source": [
    "title_bow = bow_transformer.transform(df[\"TITLE\"])\n",
    "print(title_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfTransformer()\n",
      "  (0, 244)\t0.35477264027465666\n",
      "  (0, 243)\t0.35477264027465666\n",
      "  (0, 229)\t0.35477264027465666\n",
      "  (0, 224)\t0.32278160612728646\n",
      "  (0, 221)\t0.32278160612728646\n",
      "  (0, 202)\t0.32278160612728646\n",
      "  (0, 171)\t0.32278160612728646\n",
      "  (0, 167)\t0.35477264027465666\n",
      "  (0, 61)\t0.28247766961965964\n",
      "  (1, 230)\t0.33258913256553785\n",
      "  (1, 222)\t0.36555219519046506\n",
      "  (1, 205)\t0.33258913256553785\n",
      "  (1, 183)\t0.36555219519046506\n",
      "  (1, 168)\t0.36555219519046506\n",
      "  (1, 160)\t0.36555219519046506\n",
      "  (1, 109)\t0.2528507396807884\n",
      "  (1, 62)\t0.29106058500399523\n",
      "  (1, 38)\t0.30920146743562676\n",
      "  (2, 230)\t0.3250877985152921\n",
      "  (2, 203)\t0.35730740045597703\n",
      "  (2, 202)\t0.3250877985152921\n",
      "  (2, 184)\t0.35730740045597703\n",
      "  (2, 176)\t0.3250877985152921\n",
      "  (2, 155)\t0.35730740045597703\n",
      "  (2, 141)\t0.3022276271355269\n",
      "  :\t:\n",
      "  (60, 213)\t0.30863342900757507\n",
      "  (60, 163)\t0.4373404919478541\n",
      "  (60, 159)\t0.4373404919478541\n",
      "  (60, 158)\t0.4373404919478541\n",
      "  (60, 69)\t0.2652884107464841\n",
      "  (60, 7)\t0.4065867120677751\n",
      "  (61, 240)\t0.28172913688096773\n",
      "  (61, 238)\t0.4387830389527568\n",
      "  (61, 220)\t0.3493685708572416\n",
      "  (61, 213)\t0.28172913688096773\n",
      "  (61, 105)\t0.37114360497648297\n",
      "  (61, 69)\t0.2421626044348431\n",
      "  (61, 36)\t0.4387830389527568\n",
      "  (61, 7)\t0.37114360497648297\n",
      "  (62, 240)\t0.38953790395898696\n",
      "  (62, 212)\t0.6066913318895633\n",
      "  (62, 191)\t0.6066913318895633\n",
      "  (62, 69)\t0.33483051981468887\n",
      "  (63, 213)\t0.7583507846117991\n",
      "  (63, 69)\t0.651846674823665\n",
      "  (64, 249)\t0.5186960446684721\n",
      "  (64, 213)\t0.3330388278379976\n",
      "  (64, 192)\t0.5186960446684721\n",
      "  (64, 69)\t0.2862662727045223\n",
      "  (64, 2)\t0.5186960446684721\n",
      "(65, 256)\n"
     ]
    }
   ],
   "source": [
    "# Use TF-IDF to remove insignificant words i.e. ones appear in too many docs or rarely occurs\n",
    "# NB: Gensim also has own tfidf model for extraction\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer=TfidfTransformer().fit(title_bow)\n",
    "print(tfidf_transformer)\n",
    "\n",
    "title_tfidf = tfidf_transformer.transform(title_bow)\n",
    "print(title_tfidf)\n",
    "print(title_tfidf.shape)\n",
    "\n",
    "# TODO: Use Tfidf to remove unimportant words that are used for bayes classification\n",
    "# Prevents overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform multinomial bayes i.e. fit essentially returns classifier\n",
    "# From bag of words (creates histogram) and lables df[\"CATEGORY\"]\n",
    "#! But why did we need TFIDF? Couldn't we just have used normal bow?\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB().fit(title_bow, df[\"CATEGORY\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b' 'b' 'b' 'b' 'b' 'b' 'b' 'b' 'b' 'b' 'b' 'e' 'e' 'e' 'e' 'e' 'e' 'e'\n",
      " 'e' 'e' 'e' 'e' 'e' 'e' 'e' 'e' 'e' 'e' 'e' 'e' 'e' 'e' 'e' 'e' 'e' 'e'\n",
      " 'e' 'e' 'e' 'e' 'e' 'e' 'e' 't' 't' 't' 't' 't' 't' 't' 't' 't' 't' 't'\n",
      " 't' 't' 't' 't' 't' 't' 't' 't' 't' 't' 't']\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "all_predictions = model.predict(title_tfidf)\n",
    "print(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11,  0,  0],\n",
       "       [ 0, 32,  0],\n",
       "       [ 0,  0, 22]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the confusion matrix of prediction\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(df[\"CATEGORY\"], all_predictions)\n",
    "\n",
    "# Understanding result\n",
    "# i.e. 11 in (0,0) means actual group 0 items predicted in cat 0 is 11"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
